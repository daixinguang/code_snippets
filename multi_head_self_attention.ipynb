{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daixinguang/code_snippets/blob/master/multi_head_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l4ykDVAdi2X"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "Paper: `Transformer` Attention is All you need (NIPS 2017)\n",
        "\n",
        "Code:\n",
        "- [官方TensorFlow实现](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n",
        "- [Pytorch实现](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py)\n",
        "\n",
        "Transformer architecture的两个核心sub-layers\n",
        "- Multi-Head Attention layer\n",
        "- Feed Forward Network layer\n",
        "\n",
        "Reference:\n",
        "- `Enzo_Mi` [Multi-Head Attention | 算法 + 代码](https://www.bilibili.com/video/BV1qo4y1F7Ep)\n",
        "- `黑白` [Transformer代码及解析(Pytorch)](https://zhuanlan.zhihu.com/p/345993564)\n",
        "- `大师兄`[详解Transformer （Attention Is All You Need） - 知乎](https://zhuanlan.zhihu.com/p/48508221)\n",
        "- `于建民` [The Illustrated Transformer【译】](https://blog.csdn.net/yujianmin1990/article/details/85221271)\n",
        "- `Jay Alammar` [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/dxg/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.3641,  0.7691, -0.1651,  ..., -0.2663,  1.9459, -2.0577],\n",
            "         [ 0.2793,  0.8363, -0.4215,  ..., -0.1447,  0.9408, -1.1444],\n",
            "         [ 1.5120,  0.2229, -0.6750,  ..., -0.1214,  0.8203, -2.2667],\n",
            "         ...,\n",
            "         [ 0.9347,  0.2070, -0.2067,  ...,  0.2668,  1.4383, -1.9678],\n",
            "         [ 0.6448,  1.1027,  0.6465,  ..., -0.9839,  1.4634, -1.6281],\n",
            "         [ 0.0942,  0.7632,  0.4352,  ..., -0.5545,  1.3510, -2.2930]],\n",
            "\n",
            "        [[ 0.7475,  0.0247,  0.0600,  ...,  0.3426,  1.3491, -1.7195],\n",
            "         [-1.0700,  0.8039,  0.3605,  ...,  0.1669,  0.9159, -1.7930],\n",
            "         [ 0.8181,  0.7471, -0.0348,  ...,  0.1656,  0.5735, -1.7630],\n",
            "         ...,\n",
            "         [ 0.9119,  0.8595, -0.5107,  ...,  1.3115,  0.5397, -1.7754],\n",
            "         [-0.5404,  0.7633,  0.0762,  ..., -0.3760,  1.0363, -1.9690],\n",
            "         [-0.2461,  0.5505,  0.5155,  ..., -0.6507,  1.4638, -2.1236]],\n",
            "\n",
            "        [[-0.0719,  0.9742,  0.2095,  ..., -0.1136,  0.7836, -1.5936],\n",
            "         [-0.6308, -0.0321,  0.0337,  ..., -0.2414,  1.6365, -1.6480],\n",
            "         [ 0.7632,  0.1918, -0.8705,  ..., -1.2595,  1.1137, -1.9279],\n",
            "         ...,\n",
            "         [ 1.1615,  0.9593,  0.0260,  ...,  1.0109,  0.8025, -1.7639],\n",
            "         [ 0.3168,  1.0308, -0.1975,  ..., -0.1780,  1.0753, -2.0309],\n",
            "         [ 0.0535, -0.1195, -0.1555,  ..., -0.4333,  1.5105, -1.8924]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.7743,  0.7817,  0.5099,  ..., -0.6278,  2.1036, -1.8643],\n",
            "         [-0.1554,  0.9868, -0.3776,  ...,  0.1763,  1.4205, -1.6437],\n",
            "         [ 1.2045,  0.5605, -0.0386,  ..., -0.5397, -0.7674, -1.1002],\n",
            "         ...,\n",
            "         [ 0.9595,  0.6487, -0.5649,  ...,  1.2948,  0.6614, -1.9193],\n",
            "         [ 0.0510,  1.2769,  0.1503,  ..., -1.5561,  1.2820, -1.3630],\n",
            "         [-0.7664,  0.2801, -0.2691,  ..., -1.1350,  1.2139, -1.6904]],\n",
            "\n",
            "        [[ 0.5007,  0.7886,  0.1168,  ..., -1.0682,  1.5167, -1.4988],\n",
            "         [ 0.6405, -0.0781, -0.3411,  ...,  0.2637,  1.9017, -1.8967],\n",
            "         [ 1.2094,  0.5720, -0.8705,  ..., -0.1289,  0.8616, -2.0179],\n",
            "         ...,\n",
            "         [ 1.5133, -0.1608, -0.3823,  ...,  0.3715,  0.5768, -1.8118],\n",
            "         [ 0.4689,  0.7213, -1.2933,  ..., -1.1517,  0.8102, -2.1391],\n",
            "         [ 0.1725,  0.9412,  0.6929,  ..., -1.4691,  1.1633, -2.2169]],\n",
            "\n",
            "        [[-0.2701,  1.3395, -0.1616,  ..., -0.3789,  0.9382, -1.7460],\n",
            "         [-0.3965,  1.0263, -0.4685,  ...,  0.0253,  1.4574, -2.1349],\n",
            "         [ 1.2954, -0.1798, -0.4892,  ..., -0.0914,  1.2905, -2.1193],\n",
            "         ...,\n",
            "         [ 0.0563,  0.9106, -0.6476,  ...,  0.7152,  1.1935, -1.3758],\n",
            "         [ 0.5612,  0.4079, -0.4987,  ..., -0.9451,  0.4679, -1.7816],\n",
            "         [-0.0760, -0.1680,  0.3848,  ..., -1.1204,  2.3629, -1.9099]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
        "src = torch.rand((10, 32, 512))\n",
        "tgt = torch.rand((20, 32, 512))\n",
        "out = transformer_model(src, tgt)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJU9lwPhdi2Z",
        "outputId": "dd12e909-f10f-4d28-8aef-8cbd245a7074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.4756, 0.6228],\n",
            "         [0.1560, 0.2808],\n",
            "         [0.4538, 0.6251],\n",
            "         [0.6369, 0.5038]]]) \n",
            " tensor([[[-0.1063,  0.1082, -0.4711,  0.1384, -0.4054, -0.7985],\n",
            "         [-0.1065,  0.1071, -0.4713,  0.1380, -0.4051, -0.7985],\n",
            "         [-0.1064,  0.1082, -0.4711,  0.1384, -0.4055, -0.7985],\n",
            "         [-0.1052,  0.1079, -0.4713,  0.1380, -0.4041, -0.7985]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, d_model, num_heads=3):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0 # d_model must be multiple of num_heads\n",
        "\n",
        "        self.linear_q = nn.Linear(dim_in, d_model)\n",
        "        self.linear_k = nn.Linear(dim_in, d_model)\n",
        "        self.linear_v = nn.Linear(dim_in, d_model)\n",
        "\n",
        "        self.scale = 1 / sqrt(d_model // d_model)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_in = x.shape # x: shape(batch, n, dim_in)\n",
        "        assert dim_in == self.dim_in\n",
        "\n",
        "        nh = self.num_heads\n",
        "        dk = self.d_model // nh\n",
        "\n",
        "        q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        v = self.linear_v(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "\n",
        "        dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "\n",
        "        att = torch.matmul(dist, x)\n",
        "        att = att.transpose(1,2).reshape(batch, n, self.d_model)\n",
        "\n",
        "        output = self.fc(att)\n",
        "\n",
        "        return output\n",
        "\n",
        "x = torch.rand((1,4,2))\n",
        "multi_head_att = MultiHeadSelfAttention(x.shape[2], 6, 3)\n",
        "output = multi_head_att(x)\n",
        "\n",
        "print(x, '\\n', output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjuKJ0F6di2b"
      },
      "outputs": [],
      "source": [
        "# 每个sub-layer的输入dimension=512, dim_in=512\n",
        "# 原论文中 d_model=512 dk=dv=64, h=8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SxSScFrdi2d"
      },
      "source": [
        "\n",
        "$$\n",
        "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeqsdkh4di2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "dist = torch.softmax(dist, dim=-1)\n",
        "att = torch.matmul(dist, x) # Attention(QKV)\n",
        "\n",
        "att = att.transpose(1,2).reshape(batch, n, self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKovuM9jdi2d",
        "outputId": "a40883f9-2305-451d-ec09-813199c94eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.4200, 0.3127, 0.5360, 0.5334],\n",
            "         [0.3127, 0.3135, 0.4851, 0.5296],\n",
            "         [0.5360, 0.4851, 0.7758, 0.8220],\n",
            "         [0.5334, 0.5296, 0.8220, 0.8949]]]) \n",
            " tensor([[[0.4200, 0.3127, 0.5360, 0.5334],\n",
            "         [0.3127, 0.3135, 0.4851, 0.5296],\n",
            "         [0.5360, 0.4851, 0.7758, 0.8220],\n",
            "         [0.5334, 0.5296, 0.8220, 0.8949]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand((1,4,2))\n",
        "\n",
        "res1 = x @ x.transpose(1,2)\n",
        "res2 = torch.matmul(x, x.transpose(1,2))\n",
        "\n",
        "print(res1, '\\n', res2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dxg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
