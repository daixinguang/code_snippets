{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daixinguang/code_snippets/blob/master/multi_head_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l4ykDVAdi2X"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "Paper: `Transformer` Attention is All you need (NIPS 2017)\n",
        "\n",
        "Code:\n",
        "- [官方TensorFlow实现](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n",
        "- [Pytorch实现](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py)\n",
        "\n",
        "Transformer architecture的两个核心sub-layers\n",
        "- Multi-Head Attention layer\n",
        "- Feed Forward Network layer\n",
        "- BBBBBBBBBB test\n",
        "\n",
        "Reference:\n",
        "- `Enzo_Mi` [Multi-Head Attention | 算法 + 代码](https://www.bilibili.com/video/BV1qo4y1F7Ep)\n",
        "- `黑白` [Transformer代码及解析(Pytorch)](https://zhuanlan.zhihu.com/p/345993564)\n",
        "- `大师兄` [详解Transformer （Attention Is All You Need） - 知乎](https://zhuanlan.zhihu.com/p/48508221)\n",
        "- `于建民` [The Illustrated Transformer【译】](https://blog.csdn.net/yujianmin1990/article/details/85221271)\n",
        "- `Jay Alammar` [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Torch 实现的Transformer\n",
        "\n",
        "import torch\n",
        "\n",
        "transformer_model = torch.nn.Transformer(d_model=512, \n",
        "                                        nhead=8, \n",
        "                                        num_encoder_layers=6, \n",
        "                                        num_decoder_layers=6, \n",
        "                                        dim_feedforward=2048,\n",
        "                                        dropout=0.1)\n",
        "\n",
        "src = torch.rand((10, 32, 512))\n",
        "tgt = torch.rand((20, 32, 512))\n",
        "out = transformer_model(src, tgt)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJU9lwPhdi2Z",
        "outputId": "dd12e909-f10f-4d28-8aef-8cbd245a7074"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, d_model, num_heads=3):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0 # d_model must be multiple of num_heads\n",
        "\n",
        "        self.linear_q = nn.Linear(dim_in, d_model)\n",
        "        self.linear_k = nn.Linear(dim_in, d_model)\n",
        "        self.linear_v = nn.Linear(dim_in, d_model)\n",
        "\n",
        "        self.scale = 1 / sqrt(d_model // d_model)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_in = x.shape # x: shape(batch, n, dim_in)\n",
        "        assert dim_in == self.dim_in\n",
        "\n",
        "        nh = self.num_heads\n",
        "        dk = self.d_model // nh\n",
        "\n",
        "        q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        v = self.linear_v(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "\n",
        "        dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "\n",
        "        att = torch.matmul(dist, x)\n",
        "        att = att.transpose(1,2).reshape(batch, n, self.d_model)\n",
        "\n",
        "        output = self.fc(att)\n",
        "\n",
        "        return output\n",
        "\n",
        "x = torch.rand((1,4,2))\n",
        "multi_head_att = MultiHeadSelfAttention(x.shape[2], 6, 3)\n",
        "output = multi_head_att(x)\n",
        "\n",
        "print(x, '\\n', output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjuKJ0F6di2b"
      },
      "outputs": [],
      "source": [
        "# 每个sub-layer的输入dimension=512, dim_in=512\n",
        "# 原论文中 d_model=512 dk=dv=64, h=8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SxSScFrdi2d"
      },
      "source": [
        "ScaledDotProductAttention 对应的公式和代码\n",
        "\n",
        "$$\n",
        "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeqsdkh4di2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "dist = torch.softmax(dist, dim=-1)\n",
        "att = torch.matmul(dist, x) # Attention(QKV)\n",
        "\n",
        "att = att.transpose(1,2).reshape(batch, n, self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Position-wise Feed Forward 对应的公式和代码\n",
        "\n",
        "$$\n",
        "\\mathrm{FFN}(x)=\\max(0,xW_1+b_1)W_2+b_2\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
        "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
        "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.w_2(F.relu(self.w_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        # add & norm\n",
        "        x += residual\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKovuM9jdi2d",
        "outputId": "a40883f9-2305-451d-ec09-813199c94eea"
      },
      "outputs": [],
      "source": [
        "x = torch.rand((1,4,2))\n",
        "\n",
        "res1 = x @ x.transpose(1,2)\n",
        "res2 = torch.matmul(x, x.transpose(1,2))\n",
        "\n",
        "print(res1, '\\n', res2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dxg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
