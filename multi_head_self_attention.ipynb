{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daixinguang/code_snippets/blob/master/multi_head_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l4ykDVAdi2X"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "Paper: `Transformer` Attention is All you need (NIPS 2017)\n",
        "\n",
        "Code:\n",
        "- [官方TensorFlow实现](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n",
        "- [Pytorch实现](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py)\n",
        "\n",
        "Transformer architecture的两个核心sub-layers\n",
        "- Multi-Head Attention layer\n",
        "- Feed Forward Network layer\n",
        "\n",
        "Reference:\n",
        "- `Enzo_Mi` [Multi-Head Attention | 算法 + 代码](https://www.bilibili.com/video/BV1qo4y1F7Ep)\n",
        "- `黑白` [Transformer代码及解析(Pytorch)](https://zhuanlan.zhihu.com/p/345993564)\n",
        "- [详解Transformer （Attention Is All You Need） - 知乎](https://zhuanlan.zhihu.com/p/48508221)\n",
        "- `于建民` [The Illustrated Transformer【译】](https://blog.csdn.net/yujianmin1990/article/details/85221271)\n",
        "- `Jay Alammar` [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/dxg/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.0862,  1.6395,  0.6158,  ..., -0.1350, -0.0343,  0.8277],\n",
            "         [-0.4080,  0.8118,  0.3556,  ..., -1.6812, -0.0687,  0.2144],\n",
            "         [-0.1890,  1.4179, -0.1665,  ..., -1.4447, -0.5229, -0.5293],\n",
            "         ...,\n",
            "         [ 0.1457,  0.7281, -0.5304,  ..., -1.7234,  0.8478,  0.8505],\n",
            "         [-0.3402,  2.2362,  0.4259,  ..., -1.0775,  0.1720,  0.7748],\n",
            "         [-0.1124,  1.3646, -0.2595,  ..., -1.3014, -0.5485,  1.4648]],\n",
            "\n",
            "        [[-0.4230,  1.9957,  0.1823,  ..., -0.0650, -0.2859, -0.0335],\n",
            "         [ 0.0601,  0.7105,  0.7234,  ..., -0.7285,  0.6098,  1.2656],\n",
            "         [-0.6270,  1.6926, -0.3442,  ..., -0.8210, -0.7875,  0.6791],\n",
            "         ...,\n",
            "         [-0.2587,  0.7681, -0.8603,  ..., -1.6751, -0.7452,  0.5740],\n",
            "         [-0.7096,  1.0769,  0.6512,  ...,  0.3322, -0.4507,  1.4602],\n",
            "         [-0.4582,  1.6992, -0.1435,  ..., -1.2184,  0.1195,  0.8606]],\n",
            "\n",
            "        [[-0.9396,  1.8701,  0.2147,  ..., -1.3849,  0.0541,  0.2041],\n",
            "         [ 0.1012,  0.4822,  0.1239,  ..., -1.1493, -0.8413,  1.0267],\n",
            "         [-0.2326,  1.9851,  0.4324,  ..., -1.2954, -0.6968,  2.0632],\n",
            "         ...,\n",
            "         [-0.7225,  0.8862, -0.2896,  ..., -1.2484, -0.8427,  0.8568],\n",
            "         [ 0.2372,  0.4913,  0.5028,  ...,  0.9215,  0.0962,  1.3024],\n",
            "         [ 0.3867,  1.8280,  0.1911,  ..., -1.1361, -1.4720,  0.6895]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0964,  2.0316,  0.1184,  ..., -1.1842, -0.7281, -0.0327],\n",
            "         [-0.6019,  1.0568,  0.7283,  ..., -0.9072, -0.0987,  0.5181],\n",
            "         [ 0.1562,  0.8980, -0.5917,  ...,  0.1693, -1.0124,  1.5411],\n",
            "         ...,\n",
            "         [-1.0062,  0.9127,  0.2410,  ..., -1.4867, -0.6977,  0.4937],\n",
            "         [-0.3879,  1.8768,  1.1172,  ...,  0.0997,  0.5545,  1.1378],\n",
            "         [-1.1149,  1.3750, -0.5812,  ..., -0.3261, -0.3162,  0.4039]],\n",
            "\n",
            "        [[-0.4999,  2.0557, -0.5321,  ..., -0.9615, -0.1132,  0.5522],\n",
            "         [-0.4707,  1.2124,  1.0752,  ..., -1.3381, -0.8870,  0.2149],\n",
            "         [-0.5386,  0.8940, -0.5191,  ..., -0.5653,  1.1478,  0.9682],\n",
            "         ...,\n",
            "         [-0.1754,  1.2358, -0.9089,  ..., -1.2813, -0.9772,  0.7459],\n",
            "         [-0.1526,  1.0570,  1.0860,  ..., -1.0125,  0.4998,  0.5537],\n",
            "         [-0.6159,  1.2898, -0.5527,  ..., -1.6218, -0.4312,  0.6377]],\n",
            "\n",
            "        [[-0.8723,  1.8910,  0.3825,  ...,  0.1453, -1.0698, -0.0636],\n",
            "         [-0.1657,  1.2369,  0.9531,  ..., -0.5526,  0.3610,  1.2565],\n",
            "         [ 0.0512,  0.9153, -0.4471,  ..., -0.3014, -0.5841,  1.1458],\n",
            "         ...,\n",
            "         [-0.6693,  1.3534, -0.2303,  ..., -1.0662, -0.3309,  0.0355],\n",
            "         [-0.4795,  1.1889, -0.0387,  ..., -1.2494, -0.0744,  0.5599],\n",
            "         [-0.3300,  1.5727, -0.1622,  ..., -1.0982, -0.9381,  0.9768]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
        "src = torch.rand((10, 32, 512))\n",
        "tgt = torch.rand((20, 32, 512))\n",
        "out = transformer_model(src, tgt)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJU9lwPhdi2Z",
        "outputId": "dd12e909-f10f-4d28-8aef-8cbd245a7074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.4756, 0.6228],\n",
            "         [0.1560, 0.2808],\n",
            "         [0.4538, 0.6251],\n",
            "         [0.6369, 0.5038]]]) \n",
            " tensor([[[-0.1063,  0.1082, -0.4711,  0.1384, -0.4054, -0.7985],\n",
            "         [-0.1065,  0.1071, -0.4713,  0.1380, -0.4051, -0.7985],\n",
            "         [-0.1064,  0.1082, -0.4711,  0.1384, -0.4055, -0.7985],\n",
            "         [-0.1052,  0.1079, -0.4713,  0.1380, -0.4041, -0.7985]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, d_model, num_heads=3):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0 # d_model must be multiple of num_heads\n",
        "\n",
        "        self.linear_q = nn.Linear(dim_in, d_model)\n",
        "        self.linear_k = nn.Linear(dim_in, d_model)\n",
        "        self.linear_v = nn.Linear(dim_in, d_model)\n",
        "\n",
        "        self.scale = 1 / sqrt(d_model // d_model)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_in = x.shape # x: shape(batch, n, dim_in)\n",
        "        assert dim_in == self.dim_in\n",
        "\n",
        "        nh = self.num_heads\n",
        "        dk = self.d_model // nh\n",
        "\n",
        "        q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "        v = self.linear_v(x).reshape(batch, n, nh, dk).transpose(1,2) # (batch,nh,n,dk)\n",
        "\n",
        "        dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "\n",
        "        att = torch.matmul(dist, x)\n",
        "        att = att.transpose(1,2).reshape(batch, n, self.d_model)\n",
        "\n",
        "        output = self.fc(att)\n",
        "\n",
        "        return output\n",
        "\n",
        "x = torch.rand((1,4,2))\n",
        "multi_head_att = MultiHeadSelfAttention(x.shape[2], 6, 3)\n",
        "output = multi_head_att(x)\n",
        "\n",
        "print(x, '\\n', output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjuKJ0F6di2b"
      },
      "outputs": [],
      "source": [
        "# 每个sub-layer的输入dimension=512, dim_in=512\n",
        "# 原论文中 d_model=512 dk=dv=64, h=8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SxSScFrdi2d"
      },
      "source": [
        "\n",
        "$$\n",
        "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeqsdkh4di2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "dist = torch.matmul(q,k.transpose(2,3)) * self.scale # (batch,nh,n,n)\n",
        "dist = torch.softmax(dist, dim=-1)\n",
        "att = torch.matmul(dist, x) # Attention(QKV)\n",
        "\n",
        "att = att.transpose(1,2).reshape(batch, n, self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKovuM9jdi2d",
        "outputId": "a40883f9-2305-451d-ec09-813199c94eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.4200, 0.3127, 0.5360, 0.5334],\n",
            "         [0.3127, 0.3135, 0.4851, 0.5296],\n",
            "         [0.5360, 0.4851, 0.7758, 0.8220],\n",
            "         [0.5334, 0.5296, 0.8220, 0.8949]]]) \n",
            " tensor([[[0.4200, 0.3127, 0.5360, 0.5334],\n",
            "         [0.3127, 0.3135, 0.4851, 0.5296],\n",
            "         [0.5360, 0.4851, 0.7758, 0.8220],\n",
            "         [0.5334, 0.5296, 0.8220, 0.8949]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand((1,4,2))\n",
        "\n",
        "res1 = x @ x.transpose(1,2)\n",
        "res2 = torch.matmul(x, x.transpose(1,2))\n",
        "\n",
        "print(res1, '\\n', res2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dxg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
